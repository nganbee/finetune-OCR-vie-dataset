{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14173426,"sourceType":"datasetVersion","datasetId":9034474}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"%%capture\nimport os, re\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n    \n!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\n!pip install jiwer\n!pip install einops addict easydict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:32:19.509021Z","iopub.execute_input":"2025-12-26T07:32:19.510028Z","iopub.status.idle":"2025-12-26T07:33:07.014068Z","shell.execute_reply.started":"2025-12-26T07:32:19.509987Z","shell.execute_reply":"2025-12-26T07:33:07.013214Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import json\n\nimport shutil\nfrom tqdm import tqdm\nfrom huggingface_hub import snapshot_download\nfrom unsloth import FastVisionModel, is_bf16_supported\nimport torch\nimport gc\nfrom transformers import AutoModel\nimport jiwer\nfrom typing import Dict, List, Any, Tuple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:33:07.016256Z","iopub.execute_input":"2025-12-26T07:33:07.016947Z","iopub.status.idle":"2025-12-26T07:33:53.156709Z","shell.execute_reply.started":"2025-12-26T07:33:07.016915Z","shell.execute_reply":"2025-12-26T07:33:53.156076Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-12-26 07:33:15.309426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766734395.521901      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766734395.585459      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766734396.183542      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766734396.183583      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766734396.183586      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766734396.183589      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")\n\nos.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = '0'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:33:53.157721Z","iopub.execute_input":"2025-12-26T07:33:53.158620Z","iopub.status.idle":"2025-12-26T07:34:11.281366Z","shell.execute_reply.started":"2025-12-26T07:33:53.158587Z","shell.execute_reply":"2025-12-26T07:34:11.280415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae635d7065aa44a2911b3afc356585c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1e29165e1c246fbabbb59cedfd82ba4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show3.jpg:   0%|          | 0.00/247k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"704f282dee57474b8f03b34ba656ddc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30a75c6769ff436eaa704b2e952922de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"LICENSE: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"029ae3d95e4d40dd8bc22d339b99d8c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/fig1.png:   0%|          | 0.00/396k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e1441a977e4f78b3a6f9b3c67e297a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README-checkpoint.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8dafa4f8baf424eaeb0d0124967b3c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show2.jpg:   0%|          | 0.00/216k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1469a62da23a44f28387e69a9741ed10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show1.jpg:   0%|          | 0.00/117k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aa1719df5104eb5af85cbc754a78014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91bb8a0ae9f64d148df227236e0b33ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"assets/show4.jpg:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b6f7f5965140f089f124ae2b6475a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_deepseek_v2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e7ead2d3284ec4a38728383668ca36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conversation.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b31593d82ea24a9f93b44ba3550ecec2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"deepencoder.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2576df448a0842d2b7555fde25c245c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eac3becd464468a8ce4d2f689ce2f0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4698e24b90bb4acf91bf823b49d1e7cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekocr.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2355733958e24856968a6d8a40025b8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekv2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0df8db0385244c791ae373971d216ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbd9f10d7c5b49239ca6587ab96e2527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f484d794fc94b45884cea33ab7be9bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"946f42866abc4fa898108d1d6ef968f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e5f3c9948774932bda345b8104dbb52"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"DATASETS_CONFIG_TEST = {\n    \"word\" : \"/kaggle/input/uit-hwdb/UIT_HWDB_word/UIT_HWDB_word/test_data\",\n    \"line\" : \"/kaggle/input/uit-hwdb/UIT_HWDB_line/UIT_HWDB_line/test_data\",\n    \"paragraph\" : \"/kaggle/input/uit-hwdb/UIT_HWDB_paragraph/UIT_HWDB_paragraph/test_data\"\n}\n\nDATASET_CONFIG_TRAIN = {\n\"word\": {\n        \"root_path\": '/kaggle/input/uit-hwdb/UIT_HWDB_word/UIT_HWDB_word/train_data'\n    },\n    \"line\": {\n        \"root_path\": '/kaggle/input/uit-hwdb/UIT_HWDB_line/UIT_HWDB_line/train_data'\n    },\n    \"paragraph\": {\n        \"root_path\": '/kaggle/input/uit-hwdb/UIT_HWDB_paragraph/UIT_HWDB_paragraph/train_data'\n    }\n}\n\nPROMPT = \"<image>\\nFree OCR.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:34:11.282594Z","iopub.execute_input":"2025-12-26T07:34:11.282886Z","iopub.status.idle":"2025-12-26T07:34:11.289416Z","shell.execute_reply.started":"2025-12-26T07:34:11.282858Z","shell.execute_reply":"2025-12-26T07:34:11.288090Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# HÃ m predict bá»™ dá»¯ liá»‡u test\ndef predict_OCR_test(model, tokenizer):\n    output_working_dir = '/kaggle/working/temp_ocr_process/'\n    all_results = {} \n    \n    \n    for dataset_type, root_dir in DATASETS_CONFIG_TEST.items():\n        print(f\"\\nPROCESSING: {dataset_type.upper()}\")\n        \n        # Láº¥y file trong dataset hiá»‡n táº¡i\n        file_list = []\n        for root, dirs, files in os.walk(root_dir):\n            for file in files:\n                if file.endswith(('.png', '.jpg', '.jpeg')):\n                    full_path = os.path.join(root, file)\n                    relative_path = os.path.relpath(full_path, root_dir) \n                    file_list.append((full_path, relative_path))\n        \n        print(f\"Found {len(file_list)} pics\")\n        \n        for i, (image_path, relative_name) in enumerate(tqdm(file_list)):\n            if i == 5: break\n            \n            # Táº¡o key -> phÃ¢n biá»‡t áº£nh nÃ y thuá»™c bá»™ nÃ o\n            unique_key = f\"{dataset_type}_{relative_name}\"\n            \n            # Táº¡o folder táº¡m\n            spec_output_path = os.path.join(output_working_dir, f\"{dataset_type}_{i}\")\n            os.makedirs(spec_output_path, exist_ok=True)\n    \n            try:\n                # Model Inference\n                model.infer(\n                    tokenizer,\n                    prompt = PROMPT,\n                    image_file = image_path,\n                    output_path = spec_output_path,\n                    base_size = 1024,\n                    image_size = 640,\n                    crop_mode = True,\n                    save_results = True,\n                    test_compress = False,\n                )\n                \n                # Äá»c káº¿t quáº£\n                content = \"\"\n                generated_files = os.listdir(spec_output_path)\n                for filename in generated_files:\n                    if filename.endswith(('.mmd', '.txt')):\n                        with open(os.path.join(spec_output_path, filename), 'r', encoding='utf-8') as f:\n                            content = f.read().strip()\n                        break\n                \n                # LÆ°u vÃ o dict tá»•ng\n                if content:\n                    all_results[unique_key] = content\n                \n            except Exception as e:\n                print(f\"Lá»—i: {e}\")\n            finally:\n                # Dá»n folder temp\n                if os.path.exists(spec_output_path):\n                    shutil.rmtree(spec_output_path)\n                    \n    return all_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:34:11.291731Z","iopub.execute_input":"2025-12-26T07:34:11.291995Z","iopub.status.idle":"2025-12-26T07:34:33.796758Z","shell.execute_reply.started":"2025-12-26T07:34:11.291965Z","shell.execute_reply":"2025-12-26T07:34:33.795864Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def evaluate_CER(merged_result_path):\n    with open(merged_result_path, 'r', encoding='utf-8') as f:\n        all_predictions = json.load(f)\n    \n    # Biáº¿n Ä‘á»ƒ bÃ¡o cÃ¡o tá»«ng pháº§n\n    report = {} \n    \n    # Duyá»‡t tá»«ng word, line, paragraph\n    for dataset_type, root_dir in DATASETS_CONFIG_TEST.items():\n        print(f\"\\nScoring dataset: {dataset_type.upper()}\")\n        \n        total_cer = 0\n        count = 0\n        \n        # Duyá»‡t qua cÃ¡c folder con \n        for root, dirs, files in os.walk(root_dir):\n            if 'label.json' in files:\n                label_path = os.path.join(root, 'label.json')\n                \n                # Load nhÃ£n gá»‘c\n                try:\n                    with open(label_path, 'r', encoding='utf-8') as f:\n                        local_labels = json.load(f)\n                except Exception as e:\n                    print(f\"Lá»—i Ä‘á»c file label táº¡i {label_path}: {e}\")\n                    continue\n    \n                # Láº¥y tÃªn folder hiá»‡n táº¡i Ä‘á»ƒ táº¡o key\n                current_folder_name = os.path.basename(root)\n    \n                # Duyá»‡t tá»«ng áº£nh trong file label nÃ y\n                for img_name, ground_truth in local_labels.items():\n                    \n                    relative_path = os.path.join(current_folder_name, img_name)\n                    unique_key = f\"{dataset_type}_{relative_path}\"\n                    \n                    # Láº¥y káº¿t quáº£ dá»± Ä‘oÃ¡n tá»« file tá»•ng\n                    prediction = all_predictions.get(unique_key)\n    \n                    if prediction is None:\n                        continue \n    \n                    # TÃ­nh CER\n                    gt_norm = str(ground_truth).strip()\n                    pred_norm = str(prediction).strip()\n                    \n                    if not gt_norm: \n                        continue\n    \n                    cer = jiwer.cer(gt_norm, pred_norm)\n                    total_cer += cer\n                    count += 1\n                    \n                    # In máº«u sai nhiá»u\n                    if cer > 0.5 and count % 100 == 0:\n                         print(f\" {unique_key} | CER: {cer:.2f}\")\n                         print(f\" GT  : {gt_norm}\")\n                         print(f\" Pred: {pred_norm}\")\n    \n        # Tá»•ng káº¿t cho tá»«ng loáº¡i dataset\n        if count > 0:\n            avg_cer = total_cer / count\n            report[dataset_type] = avg_cer\n\n    return report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:34:33.797805Z","iopub.execute_input":"2025-12-26T07:34:33.798081Z","iopub.status.idle":"2025-12-26T07:34:33.815016Z","shell.execute_reply.started":"2025-12-26T07:34:33.798052Z","shell.execute_reply":"2025-12-26T07:34:33.814418Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Base Model","metadata":{}},{"cell_type":"code","source":"# Get base model\nmodel_base, tokenizer_base = FastVisionModel.from_pretrained(\n    \"./deepseek_ocr\",\n    load_in_4bit = False,\n    auto_model = AutoModel,\n    trust_remote_code=True,\n    unsloth_force_compile=True,\n    use_gradient_checkpointing = \"unsloth\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:34:33.815969Z","iopub.execute_input":"2025-12-26T07:34:33.816224Z","iopub.status.idle":"2025-12-26T07:34:48.322708Z","shell.execute_reply.started":"2025-12-26T07:34:33.816195Z","shell.execute_reply":"2025-12-26T07:34:48.322103Z"}},"outputs":[{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.9: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at ./deepseek_ocr and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# LÆ°u káº¿t quáº£ vÃ o file json\nresult_path = '/kaggle/working/base_labels.json'\nall_results = predict_OCR_test(model_base, tokenizer_base)\n\nwith open(result_path, 'w', encoding='utf-8') as f:\n    json.dump(all_results, f, ensure_ascii=False, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:34:48.323562Z","iopub.execute_input":"2025-12-26T07:34:48.323800Z","iopub.status.idle":"2025-12-26T07:36:57.951646Z","shell.execute_reply.started":"2025-12-26T07:34:48.323773Z","shell.execute_reply":"2025-12-26T07:36:57.950811Z"}},"outputs":[{"name":"stdout","text":"\nPROCESSING: WORD\nFound 2881 pics\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/2881 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"\\[\\lim_{n \\to \\infty} \\frac{1}{n}\\]\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 1/2881 [00:08<7:05:06,  8.86s/it]","output_type":"stream"},{"name":"stdout","text":"\\[k ^ { 2 }\\]\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 2/2881 [00:09<3:18:30,  4.14s/it]","output_type":"stream"},{"name":"stdout","text":"\\[m ^ { \\prime }\\]\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 3/2881 [00:10<2:04:42,  2.60s/it]","output_type":"stream"},{"name":"stdout","text":"\\[\\mathrm{lim} \\tilde{\\Delta}\\]\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 4/2881 [00:11<1:31:42,  1.91s/it]","output_type":"stream"},{"name":"stdout","text":"vay\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 5/2881 [00:11<1:53:10,  2.36s/it]","output_type":"stream"},{"name":"stdout","text":"\nPROCESSING: LINE\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Found 201 pics\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/201 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nanh thÃ nh Ä‘Ã n. ÄÃ³ lÃ  thay viá»‡c. Thay thÃ nh giáº£m tÃ­n Æ°u tháº¿. Tháº­t pháº£i mÃ  mÃ¬nh.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 1/201 [00:05<19:20,  5.80s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nwhich is less than the other, but the difference is very small.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  1%|          | 2/201 [00:07<10:59,  3.31s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nmáº¹ thÃ nh máº¹ nÃªn Hai Ä‘en quÃ½ 200.000. Sand vÃ  máº¹ lÃ  Hai,\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  1%|â–         | 3/201 [00:09<09:21,  2.83s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nÄ‘á»ƒ tá»‘i 100 thÃ¡ng cÃ¡c 0 thÃ¡ng lÃ m cÃ´ng A5 10 thÃ¡ng Ä‘Ã³ng. ÄÃ³ng, Ä‘Ã£ ngÆ°á»i Ä‘Ã³ng\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  2%|â–         | 4/201 [00:12<08:53,  2.71s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nxÃºy dÃ¹ng tÄƒng vÃ³c lÃ  má»Ÿ rá»™ng pháº©m vÃ  gai pháº¡m cá»§a cÃ´ng trÃ¬nh dÃ¢y\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  2%|â–         | 5/201 [00:14<09:29,  2.90s/it]","output_type":"stream"},{"name":"stdout","text":"\nPROCESSING: PARAGRAPH\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Found 31 pics\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/31 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([3, 100, 1280])\n=====================\nPhÃ¢n tÃ­ch má»i chuyá»‡n chuyá»‡n khÃ´ng! Tham sá»‘t nhÃ¢n chuyá»‡n cÃ²n Ã¡p dá»¥ng  \n20 ngÃ y. Váº­y mÃ  hÃ£i nhÆ° ai hay chuyá»‡n Ä‘á»u khÃ¡c. ChÃº Ã½ anh Duy váº«n khÃ´ng  \nngu viáº¿t. ThÃ¢n pháº­n, huáº¥n luyá»‡n cÃ²n hÃ² hÃ²i, vÃ  dÃ¹m mÃ¹i má»›i cÅ©ng  \nváº«n lÃ m chá»© anh khÃ´ng khá»i phÃ­a. ÄÃ³ xa xa mÃ  anh Ä‘Ã£ nÃ³i: \"á»’ lÃ  lÃ   \nvÃ´ con há»“n bÃ¬nh pháº£i khÃ´ng! Láº§n nÃ o cáº§n Ä‘á»i con mÃ  pháº£i nhá»›: No / VÃ´ há»“n.  \nÄ‘á»ƒ báº£n diá»‡n Ä‘á»©c Bá»“ há»“n tÃ¢m mua chÃºt hÆ¡n, Ä‘Ã¢y lÃ  má»™t tÃ¢m ngá»™m há»“i  \ncá»§a há»“ Ä‘á»§ lá»i tiÃªn vÃ´ há»“n. HÃ£i tÃ¢m lÃ  ngá»¥a Ä‘Ã¡ng bÃ¡m bá» Ä‘á»§ kiáº¿n láº¡c  \ncháº¥t / ngá»™m ngá»¥.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  3%|â–Ž         | 1/31 [00:13<06:47, 13.57s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([3, 100, 1280])\n=====================\nChÃºng tÃ´i cáº§n thÃªm nhiá»u vá»‹ nhiÃªn (chÆ°Æ¡ng) nhá»¯ng viá»‡c lÃ m lÃ¢u cÃ²n váº«n háº¡n  \nxÃ¢y dá»±ng trong viá»‡c tháº¿ máº¡nh pháº©m vÃ  gÃ³p pháº§n cá»§a cÃ´ng nghiá»‡p,  \nhá»‡ thá»‘ng AS (AS) nhá»¯ng chá»©c láº­p thÃ nh cÆ¡ báº£n (chÆ°Æ¡ng) vÃ  thá»±c  \nÄ‘oÃ¡n (chá»©c láº­p thÃ nh cÆ¡ báº£n) Ä‘Ã£ vá»«a má»›i Ä‘Ã¢y (chÆ°Æ¡ng) káº¿ tiáº¿p lÃ½ giáº£i  \nnhá»¯ng ná»™i tháº¥t (Ä‘Ã£ cÃ³ má»¥c tiÃªu) vÃ  cÆ¡ báº£n (chÆ°Æ¡ng) thá»±c hiá»‡n (chá»©c  \nnghiá»‡m) NÄƒm lÃ m (Ä‘Ã£ cÃ³ chÆ°Æ¡ng) má»¥c tiÃªu (chÆ°Æ¡ng) Ä‘á»ƒ phá»¥c  \nvá»¥ ngÆ°á»i dÃ¢n sá»± cháº¡y cháº¡y pháº£i Ä‘áº¡t Ä‘Æ°á»£c cho má»i láº§n má»Ÿ má»Ÿ.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  6%|â–‹         | 2/31 [00:24<05:55, 12.27s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([4, 100, 1280])\n=====================\nVui lÃ²ng viáº¿t vá» má»¥c Ä‘Ã­ch cá»§a báº¡n. NgÃ y 1/1/2023, báº¡n Ä‘Ã£ báº¯t Ä‘áº§u tham gia cuá»™c thi bÃ³ng Ä‘Ã¡ vÃ  Ä‘Ã£ tham gia má»™t cuá»™c thi bÃ³ng Ä‘Ã¡ vá»›i tá»•ng sá»‘ Ä‘iá»ƒm lÃ  150.000 Ä‘á»“ng. Máº·c dÃ¹ báº¡n Ä‘Ã£ tham gia cuá»™c thi bÃ³ng Ä‘Ã¡ vá»›i tá»•ng sá»‘ Ä‘iá»ƒm lÃ  200.000 Ä‘á»“ng, báº¡n Ä‘Ã£ tham gia cuá»™c thi bÃ³ng Ä‘Ã¡ vá»›i tá»•ng sá»‘ Ä‘áº¥u tháº§u lÃ  150.000 Ä‘á»“ng.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n 10%|â–‰         | 3/31 [00:32<04:43, 10.14s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nva bÃ n nÃ y, chÃºng ta cÃ¹ng cÃ²n hÆ¡i cÃ²n háº³n váº­y vÃ  chÃ­nh mÃ¬nh.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n 13%|â–ˆâ–Ž        | 4/31 [00:34<03:09,  7.03s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([3, 100, 1280])\n=====================\nLÃ  mÃ¹a, tháº§m tÃ²a Ä‘Ã£ xÃ¡c Ä‘á»‹nh sai pháº¡m má»i thÃ´ng xá»­ lÃ½ mÃ  cÃ³ \"má»™c\" lÃºc ngay Ä‘Ã¢u? VÃ  tháº§m tÃ²a Ä‘Ã£ nghe xin lá»i \"má»™c\" cá»§a há», nhÆ°ng mÃ¬nh khÃ´ng biáº¿t Ä‘Ã¢u. VÃ  tháº§m tÃ²a Ä‘Ã£ nghe xin lá»i \"má»™\" cá»§a há», nhÆ°ng mÃ¬nh khÃ´ng biáº¿t Ä‘Ã¢u. VÃ  tháº§t tÃ²a Ä‘Ã£ nghe xin lá»i \"má»™c\" cá»§a há». VÃ  tháº§m tÃ²a Ä‘Ã£ nghe xin lá»i \"má»™t\" cá»§a há». VÃ  tháº§m tÃ²a Ä‘Ã£ nghe xin \"má»™t\" cá»§a há». VÃ  tháº§m tÃ²a Ä‘Ã£ nghes xin lá»i \"má»™t\" cá»§a há». VÃ  tháº§m tÃ² a Ä‘Ã£ nghe xin lá»i \"má»™t\" cá»§a há». VÃ  thÃ m tÃ²a Ä‘Ã£ nghe xin lá»i \"má»™t\" cá»§a há» vÃ  tháº§m tÃ²a Ä‘Ã£ nghe xin lá»i \"má»™t\" cá»§a Há». VÃ  tháº§m tÃ²a Ä‘Ã£ nghe xin lá»i \"Má»™t\" cá»§a há». VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lá»i \"Má»™t\" cá»§a há». VÃ  tháº§m tÃ²a Ä‘a nghe xin lá»i \"Má»™t\" cá»§a há». VÃ  tháº§m toa Ä‘Ã£ nghe xin lá»i \"Má»™t\" cá»§a há». VÃ  Tháº§m tÃ²a Ä‘Ã£ nghe xin lá»i \"Má»™t\" cá»§a há»™. VÃ  tháº§m tÃ²a Ä‘Ã£ nghe xin lá»i \"Má»™c\" cá»§a há». VÃ  tháº§m tÃ²a Ä‘Ã£ nghe \"Má»™t\" cá»§a há». VÃ  tháº§m tÃ²a Ä‘Ã£ nghes x in lá»i \"Má»™t\" cá»§a há». VÃ  tháº§m tÃ² a Ä‘Ã£ nghe x in lá»i \"Má»™t\" cá»§a há». VÃ  thÃ m tÃ²a Ä‘Ã£ nghe x in lá»i \"Má»™t\" cá»§a há» vÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lá»i \"Má»™t\" cá»§a Há». VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lá»i \"má»™t\" cá»§a há» vÃ  tháº§m tÃ²a Ä‘Ã£ nghes x in lá»i \"Má»™t\" cá»§a Há». VÃ  tháº§m tÃ² a Ä‘Ã£ nghe x in lá»i \"má»™t\" cá»§a há» vÃ  tháº§m toa Ä‘Ã£ nghe x in lá»i \"Má»™t\" cá»§a Há» vÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lá»i \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lá»i \"Má»™t\". VÃ¬ váº­y, tháº§m tÃ²a Ä‘Ã£ nghe x in lá»i \"Má»™t\". Váº­y, tháº§m tÃ²a Ä‘Ã£ nghe x in lá»i \"Má»™t\" cá»§a \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lá» \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lá»£i \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lÆ¡i \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in loi \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in loi \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lÃ²i \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in lÃ¶i \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe x in liÃ²i \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe xin lÃ²i \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghe xin lá»£i \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘Ã£ nghes xin lá»£i \"Má»™t\". VÃ  tháº§m tÃ²a Ä‘a nghes xin lá»£i \"Má»™t\". VÃ  tháº§m tÃ² a Ä‘Ã£ nghes xin lá»£i \"Má»™t\". VÃ  tháº§m toa Ä‘Ã£ nghes xin lá»£i \"Má»™t\". VÃ  tháº§ m tÃ²a Ä‘Ã£ nghes xin lá»£i \"Má»™t\". VÃ  Tháº§m tÃ²a Ä‘Ã£ nghes xin lá»£i \"Má»™t\".\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n 16%|â–ˆâ–Œ        | 5/31 [01:31<07:54, 18.26s/it]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"report = evaluate_CER(result_path)\n\nprint(\"\\n\" + \"-\"*30)\nprint(\"FINAL CER REPORT\")\nprint(\"-\"*30)\nfor dtype, score in report.items():\n    print(f\"{dtype:<10}: {score:.4f} ({score*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:36:57.952950Z","iopub.execute_input":"2025-12-26T07:36:57.953222Z","iopub.status.idle":"2025-12-26T07:36:59.530265Z","shell.execute_reply.started":"2025-12-26T07:36:57.953190Z","shell.execute_reply":"2025-12-26T07:36:59.529537Z"}},"outputs":[{"name":"stdout","text":"\nScoring dataset: WORD\n\nScoring dataset: LINE\n\nScoring dataset: PARAGRAPH\n\n------------------------------\nFINAL CER REPORT\n------------------------------\nword      : 6.2167 (621.67%)\nline      : 0.5638 (56.38%)\nparagraph : 0.9862 (98.62%)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"try:\n    del model_base\n    del tokenizer_base\nexcept:\n    pass\n    \n# XÃ³a cÃ¡c biáº¿n khÃ´ng dÃ¹ng\ngc.collect()\n\n# Xáº£ VRAM\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:36:59.531443Z","iopub.execute_input":"2025-12-26T07:36:59.532122Z","iopub.status.idle":"2025-12-26T07:37:00.582815Z","shell.execute_reply.started":"2025-12-26T07:36:59.532093Z","shell.execute_reply":"2025-12-26T07:37:00.582144Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Finetune Model","metadata":{}},{"cell_type":"code","source":"model_ft, tokenizer_ft = FastVisionModel.from_pretrained(\n    \"imbee510/lora_model\",\n    load_in_4bit = False,\n    auto_model = AutoModel,\n    trust_remote_code=True,\n    unsloth_force_compile=True,\n    use_gradient_checkpointing = \"unsloth\",\n)\n\nFastVisionModel.for_inference(model_ft)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:37:00.583730Z","iopub.execute_input":"2025-12-26T07:37:00.583999Z","iopub.status.idle":"2025-12-26T07:37:26.169879Z","shell.execute_reply.started":"2025-12-26T07:37:00.583971Z","shell.execute_reply":"2025-12-26T07:37:26.169038Z"}},"outputs":[{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.9: Fast Deepseekocr patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at ./deepseek_ocr and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/311M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77060735bdfb4eb686d663f89a958239"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): DeepseekOCRForCausalLM(\n      (model): DeepseekOCRModel(\n        (embed_tokens): Embedding(129280, 1280)\n        (layers): ModuleList(\n          (0): DeepseekV2DecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): DeepseekV2MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=6848, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=6848, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=6848, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=6848, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=6848, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=6848, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n            (post_attention_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n          )\n          (1-11): 11 x DeepseekV2DecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1280, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1280, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): DeepseekV2MoE(\n              (experts): ModuleList(\n                (0-63): 64 x DeepseekV2MLP(\n                  (gate_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1280, out_features=896, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1280, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=896, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (up_proj): lora.Linear(\n                    (base_layer): Linear(in_features=1280, out_features=896, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1280, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=896, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (down_proj): lora.Linear(\n                    (base_layer): Linear(in_features=896, out_features=1280, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Identity()\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=896, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1280, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (act_fn): SiLU()\n                )\n              )\n              (gate): DeepseekV2MoEGate()\n              (shared_experts): DeepseekV2MLP(\n                (gate_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1792, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1792, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1792, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1792, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1792, out_features=1280, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1792, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): SiLU()\n              )\n            )\n            (input_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n            (post_attention_layernorm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n          )\n        )\n        (norm): DeepseekV2RMSNorm((1280,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n        (sam_model): ImageEncoderViT(\n          (patch_embed): PatchEmbed(\n            (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n          )\n          (blocks): ModuleList(\n            (0-11): 12 x Block(\n              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (attn): Attention(\n                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n                (proj): Linear(in_features=768, out_features=768, bias=True)\n              )\n              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n              (mlp): MLPBlock(\n                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n                (act): GELU(approximate='none')\n              )\n            )\n          )\n          (neck): Sequential(\n            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): LayerNorm2d()\n            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (3): LayerNorm2d()\n          )\n          (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        )\n        (vision_model): VitModel(\n          (embeddings): CLIPVisionEmbeddings(\n            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n            (position_embedding): Embedding(257, 1024)\n          )\n          (transformer): NoTPTransformer(\n            (layers): ModuleList(\n              (0-23): 24 x NoTPTransformerBlock(\n                (self_attn): NoTPAttention(\n                  (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                )\n                (mlp): NoTPFeedForward(\n                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n                )\n                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              )\n            )\n          )\n          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (projector): MlpProjector(\n          (layers): Linear(in_features=2048, out_features=1280, bias=True)\n        )\n      )\n      (lm_head): Linear(in_features=1280, out_features=129280, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"result_path = '/kaggle/working/finetune_labels.json'\nall_results = predict_OCR_test(model_ft, tokenizer_ft)\n\nwith open(result_path, 'w', encoding='utf-8') as f:\n    json.dump(all_results, f, ensure_ascii=False, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:37:26.170918Z","iopub.execute_input":"2025-12-26T07:37:26.171247Z","iopub.status.idle":"2025-12-26T07:39:26.256406Z","shell.execute_reply.started":"2025-12-26T07:37:26.171206Z","shell.execute_reply":"2025-12-26T07:39:26.255620Z"}},"outputs":[{"name":"stdout","text":"\nPROCESSING: WORD\nFound 2881 pics\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/2881 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"háº¡n \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 1/2881 [00:01<1:03:35,  1.32s/it]","output_type":"stream"},{"name":"stdout","text":"há»‡ \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 2/2881 [00:02<1:03:12,  1.32s/it]","output_type":"stream"},{"name":"stdout","text":"xá»­ \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 3/2881 [00:03<1:01:58,  1.29s/it]","output_type":"stream"},{"name":"stdout","text":"hÃ¬nh \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 4/2881 [00:05<1:01:30,  1.28s/it]","output_type":"stream"},{"name":"stdout","text":"váº­y \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 5/2881 [00:06<1:01:31,  1.28s/it]","output_type":"stream"},{"name":"stdout","text":"\nPROCESSING: LINE\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Found 201 pics\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/201 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nanh chÃ­nh Ä‘Æ¡n Äá»—i hay viá»‡c thay chiáº¿m \" giáº£m \" tÃ­n cá»© \" lÃ½ phÃ¡p mÃ  \" giáº£m \". \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  0%|          | 1/201 [00:05<17:18,  5.19s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nnhiá»u kim Ä‘áº¡c phÃ­a trÃªn báº£y bá»™ nÆ¡i chÃ´n láº¿, cÅ©ng hÆ¡i \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  1%|          | 2/201 [00:09<15:29,  4.67s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nmáº·t thanh niÃªn tÃªn HÃ  Ä‘áº¿n quÄ© 200.000 Ä‘. SanDisk mua lÃ  nÃ y, \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  1%|â–         | 3/201 [00:13<14:34,  4.42s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nÄ‘á»ƒ tá»• chá»©c chiáº¿n cá» Ä‘Æ°Æ¡ng láº§n cá»™ng A5 10 tuáº§n Ä‘á»“ng. ÄÃ¢y, Ä‘Ã£ ngÆ°á»i dÃ¢n \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  2%|â–         | 4/201 [00:18<15:07,  4.60s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nxÃ¢y dá»±ng bÄƒng vÃ³c \" Ä‘i má»—i xÃ³m phá»“n \" anh giÃ  pháº¡m cá»§a cÃ´ng trÃ¬nh Ä‘Æ°á»ng \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  2%|â–         | 5/201 [00:22<14:58,  4.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nPROCESSING: PARAGRAPH\nFound 31 pics\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/31 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([3, 100, 1280])\n=====================\nPhÃ¢n tÃ­ch má»‘i cáº£nh chuyá»ƒn Ä‘á»™ng hÃ ng khÃ´ng, nhÃ¢n viÃªn chuyÃªn lÃ m \" yáº¿n \" gá»— 20 ngÆ°á»i Vá»¥ mÃ  háº§u nhÆ° ra Ã¡nh sÃ¡ng lÃªn bá»‡nh. Chá»§ láº¡nh Du Vinh hoÃ n ngÃ n NH2. ThÃ¢n pháº­n, xuáº¥t sáº¯c cá»§a há» vÃ¬ Ä‘Ã£ trÃ´i trÆ°á»£t trÃªn má»™t cáº£ng nhÆ° bá»‘n cÃ¡nh phÃ¡o háº£i Ä‘áº£o xÃ£ NhÆ° vá»‹ Ä‘á»‹a phÆ°Æ¡ng chá»‰ rÆ¡i vÃ o cÆ¡n tÃ n pháº«n láº¯m vÃ  cáº£nh Ä‘áº£o Nam phÃ­a NhÃ  nÃ³i VÃ¬ há» Ä‘Ã£ bÃ¡m Ä‘Æ°á»£c Ä‘Æ°á»£c Äá»—i Báº£o hiáº¿n tráº­n tráº¡i, cháº¯c cháº¯n Ä‘á»‹a bÃ n tráº­n ngáº­p khá»i cÆ¡n háº§u háº¿t cáº£ tÃ¬nh váº¹o ngá»±a Ä‘ang sáº¡m bÃª Ä‘i khu vá»±c chÃ¢u cháº¥p, ngáº­p ngÃ . \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  3%|â–Ž         | 1/31 [00:21<10:57, 21.91s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([3, 100, 1280])\n=====================\nChÃºng ta cÃ³ hoÃ n nghÃªnh vÃ  kiÃªn trÃ¬ chÆ°Æ¡ng trÃ¬nh viÃªn lÃ m láº¥p cá»§a cÃ´ng nhÃ¢n xÃ¢y dá»±ng bá»n vá»¯ng vá»›i áº¥n tÆ°á»£ng cá»§a cÃ´ng trÃ¬nh Ä‘Æ°á»ng, háº¿t cÃ´ng trÃ¬nh (ASB) nhá»¯ng chÃ¢n bÃ³ng cá» binh cá» biá»ƒn (thuáº­n hoÃ n nghiá»‡p vá» xÃ£) tá»± nguyá»‡n chuyá»ƒn chá»©c chá»©c hÃ nh viÃªn cá»§a ÄDV nÃ³i nhÆ°ng Ä‘áº§y nghiá»‡p káº¿ hoáº¡ch tá»± nguyá»‡n nhÃ¬n láº¡i pháº£n (Ä‘Ã£ cÃ³ muá»‘n muá»‘n) vÃ  tá»± kiá»ƒm tháº©m biá»ƒu hiá»‡n pháº£i Ä‘Æ°á»£c pháº£i lÃ m cho xÃ£ pháº£i Ä‘áº¡t Ä‘Æ°á»£c cho em lÃ m má»™t mÃ³n Äƒn ngÆ°á»i bÃ¬nh thÆ°á»ng. \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n  6%|â–‹         | 2/31 [00:39<09:25, 19.49s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([4, 100, 1280])\n=====================\nViáº¿t tiáº¿p vá» vá»¥ anh TrÆ°Æ¡ng XuÃ¢n Äáº¡i. NgÃ y tÆ° khi tá»• báº¡o \" cÃ³ pháº§n cá»§a nhá»¯ng ngÆ°á»i máº¥t lÃªn sá»± tháº­t \" pháº£i hÃ¬nh, nhiá»u láº§n Ä‘á»c Ä‘Æ°a báº£y tá» sÆ° chia sáº», Ã´ng há»™ anh TrÆ°Æ¡ng XuÃ¢n Äáº¡i. Máº·t bÃ n Ä‘á»c 67 tuá»•i á»Ÿ Q. Nhiá»u \" giáº£ \" trong anh TrÆ°Æ¡ng XuÃ¢n Äáº¡i 150.000 Ä‘á»“ng, máº¥t 6 giá» trÃªn Nhung á»Ÿ TÃ¢n BÃ¬nh gá»­i tá»•ng 100.000 Ä‘á»“ng, máº·t thÃ nh niÃªn tÃªn HÃ  \" Ä‘Ã¨n quÃ¡n 200.000 Ä‘á»“ng. Sau Ä‘Ã³ mua láº¡i \" báº£y \" nhiá»u láº§n Ä‘Ã£ phÃ¡t biá»ƒu \" báº£y \" Äá»§ chi cho 2, cÅ©ng há»™ \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n 10%|â–‰         | 3/31 [00:59<09:04, 19.43s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([9, 100, 1280])\n=====================\nvÃ  háº§m nÃ y, chÃºng ta cÅ©ng cáº§n cÃ¢n nháº¯c Ä‘áº¿n vá»¥ \" chÃ­nh mÃ¬nh \". \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n 13%|â–ˆâ–Ž        | 4/31 [01:03<06:02, 13.44s/it]","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 256, 1280])\nPATCHES:  torch.Size([3, 100, 1280])\n=====================\nLau ná»¯a, khi thÃ nh táº¡o Ä‘Ã£ xÃ¡c Ä‘inh sai pháº¡m, sá»‘ khÃ´ng má»i Ã½ nghÄ©a, \" ngÃ y má»›i cá»© \" ngÃ y 7 rá»“i tháº­m chÃ­ rá»“i Ä‘Ã£ ngÆ°a trong lÄ©nh vá»±c chuyÃªn mÃ´n, nhÆ°ng cÃ³ \" chá»§ hÃ¬nh dung Ã´ng nhÆ° thÃªm váº¹n lÃ£nh A5, cháº¯c nhÆ° tháº¿. PhÃ¡p vá»¥ pháº£i Ä‘ang cÆ°á»›i, cÃ³ \" chá»§ Ä‘áº§u tÆ° \" khu dá»± xuáº¥t tÃ³m tháº£m. Váº¡n thá»§ thÃ´ng tháº­t, phá»©c táº¡p pháº£i pháº£i má»Ÿ rá»™ng Ä‘á»ƒ \" con Ä‘Æ°á»ng xuáº¥t sáº¯c \" nhÆ° váº­y. \" Pháº£i \" gÃ³p thÃªm pháº§n ngoáº¡i xin / Ä‘i mua xuÃ¢n Ä‘Ã£ rÆ¡i cá»§a ChÃ­nh phá»§, thay tháº¿, nháº­n láº¡i vá»¥ cÃ´ng trÃ¬nh, nghÄ©a vá»¥ viá»‡c mÃ  Ä‘i báº¡o nghiÃªn cá»©u hÆ¡n vÃ¬ cÃ¡c khiáº¿m nÃ£o cÅ©ng Ä‘Ã£ ngÆ°a lao Ä‘á»™ng - dÃ²ng bÆ¡ - há» pháº£i \". \n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"\nimage: 0it [00:00, ?it/s]\u001b[A\n\nother: 0it [00:00, ?it/s]\u001b[A\n 16%|â–ˆâ–Œ        | 5/31 [01:29<07:44, 17.85s/it]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"result_path = '/kaggle/working/finetune_labels.json'\n\nreport = evaluate_CER(result_path)\n\nprint(\"\\n\" + \"-\"*30)\nprint(\"FINAL CER REPORT\")\nprint(\"-\"*30)\nfor dtype, score in report.items():\n    print(f\"{dtype:<10}: {score:.4f} ({score*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T07:39:26.257658Z","iopub.execute_input":"2025-12-26T07:39:26.258048Z","iopub.status.idle":"2025-12-26T07:39:28.118044Z","shell.execute_reply.started":"2025-12-26T07:39:26.258019Z","shell.execute_reply":"2025-12-26T07:39:28.117283Z"}},"outputs":[{"name":"stdout","text":"\nScoring dataset: WORD\n\nScoring dataset: LINE\n\nScoring dataset: PARAGRAPH\n\n------------------------------\nFINAL CER REPORT\n------------------------------\nword      : 0.4500 (45.00%)\nline      : 0.4155 (41.55%)\nparagraph : 0.4286 (42.86%)\n","output_type":"stream"}],"execution_count":13}]}