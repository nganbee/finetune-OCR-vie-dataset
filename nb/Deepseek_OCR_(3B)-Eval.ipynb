{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"03486ecc6fed4a7cb7e2e02dca0dee5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0890359ee9574b19b1b52a4c740fad41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1286473bfcdb4d5f96bc3154c26d72cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"135c7d5ae19a46ee811002c6a139692c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d05bb6c47dc460381c2a6af5266d875":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246d3453758f4803b034aff39adaa8e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acfbe50fd14240cab4e34052d6f006b9","placeholder":"​","style":"IPY_MODEL_f83610de6c30471ab5d54786fe6236c7","value":" 179999/179999 [00:08&lt;00:00, 67918.40 examples/s]"}},"271457a1a6a2478880ef17c92139a4a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d2947e714644297833248cdbbac3216":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d74cb6cfd63483d99cf994bbecf53ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3088b111494643b6bea2575b95888ac7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31f339d93fb044599e575f7971bb890b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89828f4b1f4449e3b52b8951a22054a0","placeholder":"​","style":"IPY_MODEL_0890359ee9574b19b1b52a4c740fad41","value":"Generating test split: 100%"}},"33a933565ab44e2588724492fc084f55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36910390b5da4091846ca5fcfbbf8a61":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36eadf48405b4d60afdd702c6eabe976":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3eaf210f8a434a36bff0ef7b61e53465":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3c060dc723d415daf54a4bc703dcea0","placeholder":"​","style":"IPY_MODEL_6a6fc018480541a7aea9b6e137f8f942","value":"README.md: 100%"}},"43bcadc5c612457d8cfb8605889e27a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43d4e788c9e3416fbd70cb9f360d7887":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a32cfd5a60a4c18b32f74805aed7f40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31f339d93fb044599e575f7971bb890b","IPY_MODEL_8a6700fbe79d40ec8261a68822c8c48f","IPY_MODEL_c31f976abb564d769ea56ebad2dc3ee3"],"layout":"IPY_MODEL_f769ee16de144e22a1ed214621e303aa"}},"58a03bffbbae4cbda871c0600a5808aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6da5f6c3780e47df92d660dd8b241386","max":179999,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7608c238f1040b886d6670d6e3ea5ee","value":179999}},"58feeb70efe34dab927a358d0d6ecc6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61096862da17447b88364e9548243145":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_03486ecc6fed4a7cb7e2e02dca0dee5d","max":57101872,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75e55e7d9e6b4e9f9c6bd2a84b811dc8","value":57101872}},"66370977f9974e2b992ed2fd4babc288":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca4aa01c89e047a5b8f18b3137ef72f4","placeholder":"​","style":"IPY_MODEL_1286473bfcdb4d5f96bc3154c26d72cc","value":" 57.1M/57.1M [00:02&lt;00:00, 25.3MB/s]"}},"678ae2e0d9b644b79150918ccfb4da14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d2947e714644297833248cdbbac3216","max":255644428,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58feeb70efe34dab927a358d0d6ecc6d","value":255644428}},"6a5e1968f6eb4e9e925a39b6003b9e96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a6fc018480541a7aea9b6e137f8f942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b4bc09dd88c4d0c9a52fb4b145d4819":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4cdee9de6cb4bd1b3e6a314ebcc3299","placeholder":"​","style":"IPY_MODEL_2d74cb6cfd63483d99cf994bbecf53ee","value":"Generating train split: 100%"}},"6da5f6c3780e47df92d660dd8b241386":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fb18a43b7f742a285a8e6bd43e83599":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d12030f83a134743bace242e6be5fdaf","IPY_MODEL_61096862da17447b88364e9548243145","IPY_MODEL_66370977f9974e2b992ed2fd4babc288"],"layout":"IPY_MODEL_a84e8c06726b4ab196f4f27020d0f22f"}},"734221f5c5aa473497b1a3345bbbc80a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75e55e7d9e6b4e9f9c6bd2a84b811dc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77d756887b7d4c07bf898ac41f3e5433":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d05bb6c47dc460381c2a6af5266d875","placeholder":"​","style":"IPY_MODEL_faae37a536fb4037a323964dfa53a991","value":"data/train-00000-of-00002.parquet: 100%"}},"7845466ec19946c5a076888a0f2341a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_36eadf48405b4d60afdd702c6eabe976","max":967,"min":0,"orientation":"horizontal","style":"IPY_MODEL_271457a1a6a2478880ef17c92139a4a1","value":967}},"78a793bfcc2f4d22ac73e1d9396cc434":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86a124143b0d44f8a19e0ed62921a15b","placeholder":"​","style":"IPY_MODEL_c80007e89d09489f8e4bcf9b3a5e9541","value":" 967/967 [00:00&lt;00:00, 99.5kB/s]"}},"7c213b3ba59344c3a67d449b7fcf09fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7e3f254d06844cd8cbdb91f986c87fd","IPY_MODEL_678ae2e0d9b644b79150918ccfb4da14","IPY_MODEL_a41a577cf40f466f9580347278875674"],"layout":"IPY_MODEL_6a5e1968f6eb4e9e925a39b6003b9e96"}},"7ca49bafcc404cc2a549128cbf2da144":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b4bc09dd88c4d0c9a52fb4b145d4819","IPY_MODEL_58a03bffbbae4cbda871c0600a5808aa","IPY_MODEL_246d3453758f4803b034aff39adaa8e4"],"layout":"IPY_MODEL_ffe8eb62369b47739676e2fe28b64ff3"}},"7ca58392065c404d8f17909085f87875":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3eaf210f8a434a36bff0ef7b61e53465","IPY_MODEL_7845466ec19946c5a076888a0f2341a5","IPY_MODEL_78a793bfcc2f4d22ac73e1d9396cc434"],"layout":"IPY_MODEL_98b5693678be4a15a377234298821458"}},"86a124143b0d44f8a19e0ed62921a15b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89828f4b1f4449e3b52b8951a22054a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a6700fbe79d40ec8261a68822c8c48f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f167376d3cbd428caade36eede87371c","max":20000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e7293a253c54849b8cf7ab7407cd194","value":20000}},"8bc322a71765490d8bc40432329befff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e7293a253c54849b8cf7ab7407cd194":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98b5693678be4a15a377234298821458":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99c02e19ab94422ea427d9e2fc9123c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43bcadc5c612457d8cfb8605889e27a5","placeholder":"​","style":"IPY_MODEL_43d4e788c9e3416fbd70cb9f360d7887","value":" 255M/255M [00:03&lt;00:00, 112MB/s]"}},"99f65c3d847448b2947aef46e8a05b33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a41a577cf40f466f9580347278875674":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d799f109f2a84e0ca8aae25d1dbfa194","placeholder":"​","style":"IPY_MODEL_8bc322a71765490d8bc40432329befff","value":" 256M/256M [00:03&lt;00:00, 93.5MB/s]"}},"a7e3f254d06844cd8cbdb91f986c87fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_734221f5c5aa473497b1a3345bbbc80a","placeholder":"​","style":"IPY_MODEL_36910390b5da4091846ca5fcfbbf8a61","value":"data/train-00001-of-00002.parquet: 100%"}},"a84e8c06726b4ab196f4f27020d0f22f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acfbe50fd14240cab4e34052d6f006b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6b6de1dcf8c427898db39203794a3a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77d756887b7d4c07bf898ac41f3e5433","IPY_MODEL_d41ae3498e51438f97d70db96e8c3765","IPY_MODEL_99c02e19ab94422ea427d9e2fc9123c8"],"layout":"IPY_MODEL_3088b111494643b6bea2575b95888ac7"}},"b7608c238f1040b886d6670d6e3ea5ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb6af5b3c5fc4bd1a249ecc33ce9687a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c31f976abb564d769ea56ebad2dc3ee3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33a933565ab44e2588724492fc084f55","placeholder":"​","style":"IPY_MODEL_dec53c8d9a9f4648837725261e8b153f","value":" 20000/20000 [00:00&lt;00:00, 55366.25 examples/s]"}},"c3c060dc723d415daf54a4bc703dcea0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c80007e89d09489f8e4bcf9b3a5e9541":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca4aa01c89e047a5b8f18b3137ef72f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d12030f83a134743bace242e6be5fdaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_135c7d5ae19a46ee811002c6a139692c","placeholder":"​","style":"IPY_MODEL_f2fc60fc5389457ab7dbd1c6a4ad4ea6","value":"data/test-00000-of-00001.parquet: 100%"}},"d41ae3498e51438f97d70db96e8c3765":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb6af5b3c5fc4bd1a249ecc33ce9687a","max":255327096,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99f65c3d847448b2947aef46e8a05b33","value":255327096}},"d799f109f2a84e0ca8aae25d1dbfa194":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dec53c8d9a9f4648837725261e8b153f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f167376d3cbd428caade36eede87371c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2fc60fc5389457ab7dbd1c6a4ad4ea6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4cdee9de6cb4bd1b3e6a314ebcc3299":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f769ee16de144e22a1ed214621e303aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f83610de6c30471ab5d54786fe6236c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"faae37a536fb4037a323964dfa53a991":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffe8eb62369b47739676e2fe28b64ff3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14173426,"sourceType":"datasetVersion","datasetId":9034474}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installation","metadata":{"id":"Py8_R4eE6sWh"}},{"cell_type":"code","source":"%%capture\nimport os, re\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n    \n!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\n!pip install jiwer\n!pip install einops addict easydict","metadata":{"id":"3HG_WkjS6sWi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport math\nimport io\n\nfrom huggingface_hub import snapshot_download\nfrom unsloth import FastVisionModel, is_bf16_supported\nimport torch\nfrom transformers import AutoModel, Trainer, TrainingArguments\nimport jiwer\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Any, Tuple\nfrom PIL import Image, ImageOps\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Unsloth\nPrepare OCR model","metadata":{"id":"9xLDGk41C7IF"}},{"cell_type":"code","source":"snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")","metadata":{"id":"lxV7W6236sWk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = '0'\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Qwen3-VL-8B-Instruct-bnb-4bit\", # Qwen 3 vision support\n    \"unsloth/Qwen3-VL-8B-Thinking-bnb-4bit\",\n    \"unsloth/Qwen3-VL-32B-Instruct-bnb-4bit\",\n    \"unsloth/Qwen3-VL-32B-Thinking-bnb-4bit\",\n]\n\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"./deepseek_ocr\",\n    load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n    auto_model = AutoModel,\n    trust_remote_code=True,\n    unsloth_force_compile=True,\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n)","metadata":{"id":"QmUBVEnvCDJv","outputId":"d378c8c8-c9ec-4c0a-beee-d2433628d23b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using Baseline Model","metadata":{"id":"JfZVh2ByxFNh"}},{"cell_type":"code","source":"import os\nimport json\nimport shutil\nfrom tqdm import tqdm\n\nDATASETS_CONFIG_TEST = {\n    \"word\" : \"/kaggle/input/uit-hwdb/UIT_HWDB_word/UIT_HWDB_word/test_data\",\n    \"line\" : \"/kaggle/input/uit-hwdb/UIT_HWDB_line/UIT_HWDB_line/test_data\",\n    \"paragraph\" : \"/kaggle/input/uit-hwdb/UIT_HWDB_paragraph/UIT_HWDB_paragraph/test_data\"\n}\n\nDATASET_CONFIG_TRAIN = {\n\"word\": {\n        \"root_path\": '/kaggle/input/uit-hwdb/UIT_HWDB_word/UIT_HWDB_word/train_data',\n        \"prompt\": \"<image>\\nOCR Word.\"\n    },\n    \"line\": {\n        \"root_path\": '/kaggle/input/uit-hwdb/UIT_HWDB_line/UIT_HWDB_line/train_data',\n        \"prompt\": \"<image>\\nOCR Line.\"\n    },\n    \"paragraph\": {\n        \"root_path\": '/kaggle/input/uit-hwdb/UIT_HWDB_paragraph/UIT_HWDB_paragraph/train_data',\n        \"prompt\": \"<image>\\nFree OCR.\"\n    }\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_working_dir = '/kaggle/working/temp_ocr_process/'\nfinal_json_path = '/kaggle/working/merged_labels.json'\n\nall_results = {} \nprompt = \"<image>\\nFree OCR.\"\n\nfor dataset_type, root_dir in DATASETS_CONFIG_TEST.items():\n    print(f\"\\nPROCESSING: {dataset_type.upper()}\")\n    \n    # Lấy file trong dataset hiện tại\n    file_list = []\n    for root, dirs, files in os.walk(root_dir):\n        for file in files:\n            if file.endswith(('.png', '.jpg', '.jpeg')):\n                full_path = os.path.join(root, file)\n                relative_path = os.path.relpath(full_path, root_dir) \n                file_list.append((full_path, relative_path))\n    \n    print(f\"Found {len(file_list)} pícs\")\n    \n    for i, (image_path, relative_name) in enumerate(tqdm(file_list)):\n        if i == 5: break\n        \n        # Tạo key -> phân biệt ảnh này thuộc bộ nào\n        unique_key = f\"{dataset_type}_{relative_name}\"\n        \n        # Tạo folder tạm\n        spec_output_path = os.path.join(output_working_dir, f\"{dataset_type}_{i}\")\n        os.makedirs(spec_output_path, exist_ok=True)\n\n        try:\n            # Model Inference\n            model.infer(\n                tokenizer,\n                prompt = prompt,\n                image_file = image_path,\n                output_path = spec_output_path,\n                base_size = 1024,\n                image_size = 640,\n                crop_mode = True,\n                save_results = True,\n                test_compress = False,\n            )\n            \n            # Đọc kết quả\n            content = \"\"\n            generated_files = os.listdir(spec_output_path)\n            for filename in generated_files:\n                if filename.endswith(('.mmd', '.txt')):\n                    with open(os.path.join(spec_output_path, filename), 'r', encoding='utf-8') as f:\n                        content = f.read().strip()\n                    break\n            \n            # Lưu vào dict tổng\n            if content:\n                all_results[unique_key] = content\n            \n        except Exception as e:\n            print(f\"Lỗi: {e}\")\n        finally:\n            # Dọn folder temp\n            if os.path.exists(spec_output_path):\n                shutil.rmtree(spec_output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lưu kết quả vào file json\nwith open(final_json_path, 'w', encoding='utf-8') as f:\n    json.dump(all_results, f, ensure_ascii=False, indent=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **CER Metric**","metadata":{}},{"cell_type":"code","source":"def evaluate_CER(merged_result_path):\n    with open(merged_result_path, 'r', encoding='utf-8') as f:\n        all_predictions = json.load(f)\n    \n    # Biến để báo cáo từng phần\n    report = {} \n    \n    # Duyệt từng word, line, paragraph\n    for dataset_type, root_dir in DATASETS_CONFIG_TEST.items():\n        print(f\"\\nScoring dataset: {dataset_type.upper()}\")\n        \n        total_cer = 0\n        count = 0\n        \n        # Duyệt qua các folder con \n        for root, dirs, files in os.walk(root_dir):\n            if 'label.json' in files:\n                label_path = os.path.join(root, 'label.json')\n                \n                # Load nhãn gốc\n                try:\n                    with open(label_path, 'r', encoding='utf-8') as f:\n                        local_labels = json.load(f)\n                except Exception as e:\n                    print(f\"Lỗi đọc file label tại {label_path}: {e}\")\n                    continue\n    \n                # Lấy tên folder hiện tại để tạo key\n                current_folder_name = os.path.basename(root)\n    \n                # Duyệt từng ảnh trong file label này\n                for img_name, ground_truth in local_labels.items():\n                    \n                    relative_path = os.path.join(current_folder_name, img_name)\n                    unique_key = f\"{dataset_type}_{relative_path}\"\n                    \n                    # Lấy kết quả dự đoán từ file tổng\n                    prediction = all_predictions.get(unique_key)\n    \n                    if prediction is None:\n                        continue \n    \n                    # Tính CER\n                    gt_norm = str(ground_truth).strip()\n                    pred_norm = str(prediction).strip()\n                    \n                    if not gt_norm: \n                        continue\n    \n                    cer = jiwer.cer(gt_norm, pred_norm)\n                    total_cer += cer\n                    count += 1\n                    \n                    # In mẫu sai nhiều\n                    if cer > 0.5 and count % 100 == 0:\n                         print(f\" {unique_key} | CER: {cer:.2f}\")\n                         print(f\" GT  : {gt_norm}\")\n                         print(f\" Pred: {pred_norm}\")\n    \n        # Tổng kết cho từng loại dataset\n        if count > 0:\n            avg_cer = total_cer / count\n            report[dataset_type] = avg_cer\n            #print(f\"Result {dataset_type}: {count} samples | Avg CER: {avg_cer:.4f}\")\n    \n\n\n    return report","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_result_path = '/kaggle/working/merged_labels.json'\n\nreport = evaluate_CER(merged_result_path)\n\nprint(\"\\n\" + \"-\"*30)\nprint(\"FINAL CER REPORT\")\nprint(\"-\"*30)\nfor dtype, score in report.items():\n    print(f\"{dtype:<10}: {score:.4f} ({score*100:.2f}%)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finetune Deepseek-OCR","metadata":{"id":"fzckMII_02s_"}},{"cell_type":"markdown","source":"We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n\n**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!","metadata":{"id":"SXd9bTZd1aaL"}},{"cell_type":"code","source":"model = FastVisionModel.get_peft_model(\n    model,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n\n    r = 16,           # The larger, the higher the accuracy, but might overfit\n    lora_alpha = 16,  # Recommended alpha == r at least\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n)","metadata":{"id":"6bZsfBuZDeCL","outputId":"8510ca96-59ba-42fa-93ea-ed6df2058e0e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Prep","metadata":{"id":"vITh0KVJ10qX"}},{"cell_type":"markdown","source":"To format the dataset, all vision finetuning tasks should be formatted as follows:\n\n```python\n[\n{ \"role\": \"<|User|>\",\n  \"content\": \"\",\n  \"images\": []\n},\n{ \"role\": \"<|Assistant|>\",\n  \"content\": \"\"\n},\n]\n```","metadata":{"id":"K9CBpiISFa6C"}},{"cell_type":"markdown","source":"### Creating formatted dataset","metadata":{}},{"cell_type":"code","source":"def create_conversation(image_path, text_label, instruction):\n    return {\n        \"messages\": [\n            {\n                \"role\": \"<|User|>\",\n                \"content\": instruction,\n                \"images\": [image_path]\n            },\n            {\n                \"role\": \"<|Assistant|>\",\n                \"content\": text_label\n            }\n        ]\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_FILE = '/kaggle/working/train_dataset.json'\n\nfinal_dataset = []\nstats = {k: 0 for k in DATASET_CONFIG_TRAIN.keys()}\n\nfor data_type, config in DATASET_CONFIG_TRAIN.items():\n    root_dir = config[\"root_path\"]\n    prompt = config[\"prompt\"]\n\n    for root, dirs, files in os.walk(root_dir):\n        if 'label.json' in files:\n            label_path = os.path.join(root, 'label.json')\n\n            try:\n                with open(label_path, 'r', encoding = 'utf-8') as f:\n                    label_data = json.load(f)\n            except Exception as e:\n                print(f\"Error: Can not open file {label_path}\")\n                continue\n\n            valid_imgs = [f for f in files if f.endswith(('.png', '.jpg', '.jpeg'))]\n            for img_name in valid_imgs:\n                if img_name in label_data:\n                    full_img_path = os.path.join(root, img_name)\n                    text_context = label_data[img_name]\n\n                    sample = create_conversation(full_img_path, text_context, prompt)\n                    final_dataset.append(sample)\n\n                    stats[data_type] += 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*30)\nprint(\"REPORT\")\nprint(\"=\"*30)\ntotal_count = 0\nfor dtype, count in stats.items():\n    print(f\"- {dtype.upper():<10}: {count} mẫu\")\n    total_count += count\nprint(\"-\" * 30)\nprint(f\"TỔNG CỘNG : {total_count} mẫu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n        json.dump(final_dataset, f, ensure_ascii=False, indent=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chuyển file json sang Dataset, chia tỉ lệ word, line, para trong train data\nTARGET_TOTAL_SAMPLES = 10000\n\nRATIOS = {\n    \"word\": 0.2,      \n    \"line\": 0.7,      \n    \"paragraph\": 0.1\n}\n\nwith open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n    all_data = json.load(f)\n\nbuckets = {\n    \"word\": [],\n    \"line\": [],\n    \"paragraph\": []\n}\n\nfor item in all_data:\n    try:\n        # Lấy đường dẫn\n        image_path = item['messages'][0]['images'][0]\n        path_lower = image_path.lower()\n\n        # Check keyword trong đường dẫn và thêm vào buckets\n        if \"word\" in path_lower:\n            buckets[\"word\"].append(item)\n            \n        elif \"line\" in path_lower:\n            buckets[\"line\"].append(item)\n\n        elif \"paragraph\" in path_lower: \n            buckets[\"paragraph\"].append(item)\n            \n        else:\n            buckets[\"paragraph\"].append(item) # Tạm gán vào paragraph\n            \n    except Exception as e:\n        print(f\"Lỗi mẫu dữ liệu: {e}\")\n        continue\n\nfor k, v in buckets.items():\n    print(f\"- {k.upper()}: {len(v)} mẫu\")\n\n# --- 2. LẤY MẪU THEO TỶ LỆ (SAMPLING) ---\nfinal_train_list = []\nrandom.seed(SEED)\n\nfor dtype, ratio in RATIOS.items():\n    # Tính số lượng cần lấy\n    n_needed = int(TARGET_TOTAL_SAMPLES * ratio)\n    \n    # Số lượng thực tế đang có\n    n_available = len(buckets[dtype])\n    \n    if n_available == 0:\n        print(f\"Không có dữ liệu loại {dtype}!\")\n        continue\n        \n    # Nếu dữ liệu có ít hơn số cần lấy -> Lấy hết những gì đang có\n    if n_available < n_needed:\n        print(f\"{dtype}: Cần {n_needed}, chỉ có {n_available}\")\n        selected_items = buckets[dtype]\n    else:\n        # Nếu dư -> random\n        print(f\"{dtype}: Lấy ngẫu nhiên {n_needed} mẫu từ {n_available}.\")\n        selected_items = random.sample(buckets[dtype], n_needed)\n        \n    final_train_list.extend(selected_items)\n\nrandom.shuffle(final_train_list) # Trộn dataset\n\ntrain_dataset = Dataset.from_list(final_train_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Datacollator","metadata":{}},{"cell_type":"code","source":"from deepseek_ocr.modeling_deepseekocr import (\n    format_messages,\n    text_encode,\n    BasicImageTransform,\n    dynamic_preprocess,\n)\n\n@dataclass\nclass DeepSeekOCRDataCollator:\n    \"\"\"\n    Args:\n        tokenizer: Tokenizer\n        model: Model\n        image_size: Size for image patches (default: 640)\n        base_size: Size for global view (default: 1024)\n        crop_mode: Whether to use dynamic cropping for large images\n        train_on_responses_only: If True, only train on assistant responses (mask user prompts)\n    \"\"\"\n    tokenizer: Any\n    model: Any\n    image_size: int = 640\n    base_size: int = 1024\n    crop_mode: bool = True\n    image_token_id: int = 128815\n    train_on_responses_only: bool = True\n\n    def __init__(\n        self,\n        tokenizer,\n        model,\n        image_size: int = 640,\n        base_size: int = 1024,\n        crop_mode: bool = True,\n        train_on_responses_only: bool = True,\n    ):\n        self.tokenizer = tokenizer\n        self.model = model\n        self.image_size = image_size\n        self.base_size = base_size\n        self.crop_mode = crop_mode\n        self.image_token_id = 128815\n        self.dtype = model.dtype  # Get dtype from model\n        self.train_on_responses_only = train_on_responses_only\n\n        self.image_transform = BasicImageTransform(\n            mean=(0.5, 0.5, 0.5),\n            std=(0.5, 0.5, 0.5),\n            normalize=True\n        )\n        self.patch_size = 16\n        self.downsample_ratio = 4\n\n        # Get BOS token ID from tokenizer\n        if hasattr(tokenizer, 'bos_token_id') and tokenizer.bos_token_id is not None:\n            self.bos_id = tokenizer.bos_token_id\n        else:\n            self.bos_id = 0\n            print(f\"Warning: tokenizer has no bos_token_id, using default: {self.bos_id}\")\n\n    def deserialize_image(self, image_data) -> Image.Image:\n        \"\"\"Convert image data (bytes dict or PIL Image) to PIL Image in RGB mode\"\"\"\n        if isinstance(image_data, str):\n            return Image.open(image_data).convert(\"RGB\")\n        \n        if isinstance(image_data, Image.Image):\n            return image_data.convert(\"RGB\")\n        elif isinstance(image_data, dict) and 'bytes' in image_data:\n            image_bytes = image_data['bytes']\n            image = Image.open(io.BytesIO(image_bytes))\n            return image.convert(\"RGB\")\n        else:\n            raise ValueError(f\"Unsupported image format: {type(image_data)}\")\n\n    def calculate_image_token_count(self, image: Image.Image, crop_ratio: Tuple[int, int]) -> int:\n        \"\"\"Calculate the number of tokens this image will generate\"\"\"\n        num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n        num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n\n        width_crop_num, height_crop_num = crop_ratio\n\n        if self.crop_mode:\n            img_tokens = num_queries_base * num_queries_base + 1\n            if width_crop_num > 1 or height_crop_num > 1:\n                img_tokens += (num_queries * width_crop_num + 1) * (num_queries * height_crop_num)\n        else:\n            img_tokens = num_queries * num_queries + 1\n\n        return img_tokens\n\n    def process_image(self, image: Image.Image) -> Tuple[List, List, List, List, Tuple[int, int]]:\n        \"\"\"\n        Process a single image based on crop_mode and size thresholds\n\n        Returns:\n            Tuple of (images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio)\n        \"\"\"\n        images_list = []\n        images_crop_list = []\n        images_spatial_crop = []\n\n        if self.crop_mode:\n            # Determine crop ratio based on image size\n            if image.size[0] <= 640 and image.size[1] <= 640:\n                crop_ratio = (1, 1)\n                images_crop_raw = []\n            else:\n                images_crop_raw, crop_ratio = dynamic_preprocess(\n                    image, min_num=2, max_num=9,\n                    image_size=self.image_size, use_thumbnail=False\n                )\n\n            # Process global view with padding\n            global_view = ImageOps.pad(\n                image, (self.base_size, self.base_size),\n                color=tuple(int(x * 255) for x in self.image_transform.mean)\n            )\n            images_list.append(self.image_transform(global_view).to(self.dtype))\n\n            width_crop_num, height_crop_num = crop_ratio\n            images_spatial_crop.append([width_crop_num, height_crop_num])\n\n            # Process local views (crops) if applicable\n            if width_crop_num > 1 or height_crop_num > 1:\n                for crop_img in images_crop_raw:\n                    images_crop_list.append(\n                        self.image_transform(crop_img).to(self.dtype)\n                    )\n\n            # Calculate image tokens\n            num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n            num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n\n            tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base\n            tokenized_image += [self.image_token_id]\n\n            if width_crop_num > 1 or height_crop_num > 1:\n                tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n                    num_queries * height_crop_num)\n\n        else:  # crop_mode = False\n            crop_ratio = (1, 1)\n            images_spatial_crop.append([1, 1])\n\n            # For smaller base sizes, resize; for larger, pad\n            if self.base_size <= 640:\n                resized_image = image.resize((self.base_size, self.base_size), Image.LANCZOS)\n                images_list.append(self.image_transform(resized_image).to(self.dtype))\n            else:\n                global_view = ImageOps.pad(\n                    image, (self.base_size, self.base_size),\n                    color=tuple(int(x * 255) for x in self.image_transform.mean)\n                )\n                images_list.append(self.image_transform(global_view).to(self.dtype))\n\n            num_queries = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n            tokenized_image = ([self.image_token_id] * num_queries + [self.image_token_id]) * num_queries\n            tokenized_image += [self.image_token_id]\n\n        return images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio\n\n    def process_single_sample(self, messages: List[Dict]) -> Dict[str, Any]:\n            \"\"\"\n            Process a single conversation into model inputs.\n            \"\"\"\n\n            # --- 1. Setup ---\n            images = []\n            for message in messages:\n                if \"images\" in message and message[\"images\"]:\n                    for img_data in message[\"images\"]:\n                        if img_data is not None:\n                            pil_image = self.deserialize_image(img_data)\n                            images.append(pil_image)\n\n            if not images:\n                raise ValueError(\"No images found in sample. Please ensure all samples contain images.\")\n\n            tokenized_str = []\n            images_seq_mask = []\n            images_list, images_crop_list, images_spatial_crop = [], [], []\n\n            prompt_token_count = -1 # Index to start training\n            assistant_started = False\n            image_idx = 0\n\n            # Add BOS token at the very beginning\n            tokenized_str.append(self.bos_id)\n            images_seq_mask.append(False)\n\n            for message in messages:\n                role = message[\"role\"]\n                content = message[\"content\"]\n\n                # Check if this is the assistant's turn\n                if role == \"<|Assistant|>\":\n                    if not assistant_started:\n                        # This is the split point. All tokens added *so far*\n                        # are part of the prompt.\n                        prompt_token_count = len(tokenized_str)\n                        assistant_started = True\n\n                    # Append the EOS token string to the *end* of assistant content\n                    content = f\"{content.strip()} {self.tokenizer.eos_token}\"\n\n                # Split this message's content by the image token\n                text_splits = content.split('<image>')\n\n                for i, text_sep in enumerate(text_splits):\n                    # Tokenize the text part\n                    tokenized_sep = text_encode(self.tokenizer, text_sep, bos=False, eos=False)\n                    tokenized_str.extend(tokenized_sep)\n                    images_seq_mask.extend([False] * len(tokenized_sep))\n\n                    # If this text is followed by an <image> tag\n                    if i < len(text_splits) - 1:\n                        if image_idx >= len(images):\n                            raise ValueError(\n                                f\"Data mismatch: Found '<image>' token but no corresponding image.\"\n                            )\n\n                        # Process the image\n                        image = images[image_idx]\n                        img_list, crop_list, spatial_crop, tok_img, _ = self.process_image(image)\n\n                        images_list.extend(img_list)\n                        images_crop_list.extend(crop_list)\n                        images_spatial_crop.extend(spatial_crop)\n\n                        # Add image placeholder tokens\n                        tokenized_str.extend(tok_img)\n                        images_seq_mask.extend([True] * len(tok_img))\n\n                        image_idx += 1 # Move to the next image\n\n            # --- 3. Validation and Final Prep ---\n            if image_idx != len(images):\n                raise ValueError(\n                    f\"Data mismatch: Found {len(images)} images but only {image_idx} '<image>' tokens were used.\"\n                )\n\n            # If we never found an assistant message, we're in a weird state\n            # (e.g., user-only prompt). We mask everything.\n            if not assistant_started:\n                print(\"Warning: No assistant message found in sample. Masking all tokens.\")\n                prompt_token_count = len(tokenized_str)\n\n            # Prepare image tensors\n            images_ori = torch.stack(images_list, dim=0)\n            images_spatial_crop_tensor = torch.tensor(images_spatial_crop, dtype=torch.long)\n\n            if images_crop_list:\n                images_crop = torch.stack(images_crop_list, dim=0)\n            else:\n                images_crop = torch.zeros((1, 3, self.base_size, self.base_size), dtype=self.dtype)\n\n            return {\n                \"input_ids\": torch.tensor(tokenized_str, dtype=torch.long),\n                \"images_seq_mask\": torch.tensor(images_seq_mask, dtype=torch.bool),\n                \"images_ori\": images_ori,\n                \"images_crop\": images_crop,\n                \"images_spatial_crop\": images_spatial_crop_tensor,\n                \"prompt_token_count\": prompt_token_count, # This is now accurate\n            }\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        \"\"\"Collate batch of samples\"\"\"\n        batch_data = []\n\n        # Process each sample\n        for feature in features:\n            try:\n                processed = self.process_single_sample(feature['messages'])\n                batch_data.append(processed)\n            except Exception as e:\n                print(f\"Error processing sample: {e}\")\n                continue\n\n        if not batch_data:\n            raise ValueError(\"No valid samples in batch\")\n\n        # Extract lists\n        input_ids_list = [item['input_ids'] for item in batch_data]\n        images_seq_mask_list = [item['images_seq_mask'] for item in batch_data]\n        prompt_token_counts = [item['prompt_token_count'] for item in batch_data]\n\n        # Pad sequences\n        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n        images_seq_mask = pad_sequence(images_seq_mask_list, batch_first=True, padding_value=False)\n\n        # Create labels\n        labels = input_ids.clone()\n\n        # Mask padding tokens\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        # Mask image tokens (model shouldn't predict these)\n        labels[images_seq_mask] = -100\n\n        # Mask user prompt tokens when train_on_responses_only=True (only train on assistant responses)\n        if self.train_on_responses_only:\n            for idx, prompt_count in enumerate(prompt_token_counts):\n                if prompt_count > 0:\n                    labels[idx, :prompt_count] = -100\n\n        # Create attention mask\n        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n\n        # Prepare images batch (list of tuples)\n        images_batch = []\n        for item in batch_data:\n            images_batch.append((item['images_crop'], item['images_ori']))\n\n        # Stack spatial crop info\n        images_spatial_crop = torch.cat([item['images_spatial_crop'] for item in batch_data], dim=0)\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n            \"images\": images_batch,\n            \"images_seq_mask\": images_seq_mask,\n            \"images_spatial_crop\": images_spatial_crop,\n        }","metadata":{"cellView":"form","id":"E2WR-p20LcG_","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train model","metadata":{"id":"idAEIeSQ3xdS"}},{"cell_type":"code","source":"FastVisionModel.for_training(model) # Bật model để train\n\ndata_collator = DeepSeekOCRDataCollator(\n    tokenizer=tokenizer,\n    model = model,\n    image_size=640,\n    base_size=1024,\n    crop_mode=True,\n    train_on_responses_only=True,\n)\n\ntrainer = Trainer(\n    model = model,\n    processing_class = tokenizer, \n    data_collator = data_collator,\n    train_dataset = converted_dataset,\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        #max_steps = 60,\n        num_train_epochs = 3, # Chạy dữ liệu 3 lần\n        learning_rate = 2e-4,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.001,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        fp16 = not is_bf16_supported(),  # Use fp16 if bf16 is not supported\n        bf16 = is_bf16_supported(),  # Use bf16 if supported\n        output_dir = \"outputs\",\n        report_to = \"none\",     # For Weights and Biases\n        dataloader_num_workers=2, # Dùng 2 nhân CPU để chạy\n        # You MUST put the below items for vision finetuning:\n        remove_unused_columns = False,\n    ),\n)","metadata":{"id":"95_Nn-89DhsL","outputId":"fbf849bf-7476-4b43-8155-77aae75efc01","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"cellView":"form","id":"2ejIt2xSNKKp","outputId":"88c63124-2589-457c-b6da-31d66aa85e34","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"id":"yqxqAZ7KJ4oL","outputId":"cbfafb64-473f-4521-c4af-aa6c643e0b80","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\n    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"cellView":"form","id":"pCqnaKmlO1U9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"Inference\"></a>\n# Inference\nLet's run the model!","metadata":{"id":"ekOmTR1hSNcr"}},{"cell_type":"code","source":"prompt = \"<image>\\nFree OCR. \"\nimage_file = '/kaggle/input/uit-hwdb-word/UIT_HWDB_word/test_data/250/1.jpg'\noutput_path = '/kaggle/working/results/'\n\n# Tiny: base_size = 512, image_size = 512, crop_mode = False\n# Small: base_size = 640, image_size = 640, crop_mode = False\n# Base: base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n\n# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file,\n    output_path = output_path,\n    image_size=640,\n    base_size=1024,\n    crop_mode=True,\n    save_results = True,\n    test_compress = False)\n","metadata":{"id":"kR3gIAX-SM2q","outputId":"b9939f3d-2d63-432a-a782-5b8619bf5254","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"Save\"></a>\n### Saving, loading finetuned models\nTo save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n\n**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!","metadata":{"id":"uMuVrWbjAzhc"}},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\")  # Local saving\ntokenizer.save_pretrained(\"lora_model\")\n# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving","metadata":{"id":"upcOlWe7A1vc","outputId":"00ea7366-5427-4301-cf04-e883cf5170ea","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:","metadata":{"id":"AEEcJ4qfC7Lp"}},{"cell_type":"code","source":"if False:\n    from unsloth import FastVisionModel\n    model, tokenizer = FastVisionModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n        auto_model = AutoModel,\n        trust_remote_code=True,\n        unsloth_force_compile=True,\n        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n    )\n    FastVisionModel.for_inference(model) # Enable for inference!\n\nprompt = \"<image>\\nFree OCR. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n\n# Tiny: base_size = 512, image_size = 512, crop_mode = False\n# Small: base_size = 640, image_size = 640, crop_mode = False\n# Base: base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n\n# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file,\n    output_path = output_path,\n    image_size=640,\n    base_size=1024,\n    crop_mode=True,\n    save_results = True,\n    test_compress = False)\n","metadata":{"id":"MKX_XKs_BNZR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Saving to float16 for VLLM\n\nWe also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.","metadata":{"id":"f422JgM9sdVT"}},{"cell_type":"code","source":"# Select ONLY 1 to save! (Both not needed!)\n\n# Save locally to 16bit\nif False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n\n# To export and save to your Hugging Face account\nif False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", tokenizer, token = \"PUT_HERE\")","metadata":{"id":"iHjt_SMYsd3P","trusted":true},"outputs":[],"execution_count":null}]}